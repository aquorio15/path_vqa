# Path_VQA
The following repository contains code for the paper 'TM-PATHVQA: 90000+ Textless Multilingual Questions for Medical Visual
Question Answering' which has been accepted in Interspeech 2024.
# Overview
The paper explores a novel VQA dataset in healthcare and medical diagnostics. Current text-based VQA systems limit their utility in scenarios where hands-free interaction and accessibility are crucial while performing tasks. A speech-based VQA system may provide a better means of interaction where information can be accessed while performing tasks simultaneously. To this end, this work implements a speech-based VQA system by introducing a Textless Multilingual Pathological VQA (TM-PathVQA) dataset, an expansion of the PathVQA dataset
The dataset can be accessed by the following [link](https://zenodo.org/records/10974246?token=eyJhbGciOiJIUzUxMiJ9.eyJpZCI6IjAxNGIyMjQ1LTk4NWYtNDFlYS04MGU1LTBmZmRjNzg4ZTYyYSIsImRhdGEiOnt9LCJyYW5kb20iOiI3OWRkYjA1MGVmYzcwNTI4YTMzZWViNDcxZjA2YzY4ZSJ9.iJwlSoVzXmshrnnr4s20dC-dcgJJDuNnDY8rk1HxLBwicoua3Ri-OsJosga9LFxRHzaEvrWAlbmcxJFfeygohQ
):
In order to replicate the baseline results in the paper kindly go through the following steps:
```Step-1
python<=3.10
torch-2.1.0
```
